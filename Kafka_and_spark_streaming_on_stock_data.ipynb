{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!tar xf /content/drive/Shareddrives/DA231-2022-public/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install yfinance\n",
        "!pip install plotly\n",
        "!pip install --upgrade pandas-datareader\n",
        "!pip install kafka-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbNsslXQd-xu",
        "outputId": "ef935a4a-4961-4481-d47a-a20236ba6846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.8/dist-packages (0.1.87)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.8/dist-packages (from yfinance) (2.28.1)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from yfinance) (4.9.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->yfinance) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.8/dist-packages (5.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly) (8.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.8/dist-packages (0.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pandas-datareader) (2.28.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.8/dist-packages (from pandas-datareader) (1.3.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from pandas-datareader) (4.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23->pandas-datareader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23->pandas-datareader) (2022.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23->pandas-datareader) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->pandas-datareader) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pandas-datareader) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pandas-datareader) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pandas-datareader) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pandas-datareader) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
            "\u001b[K     |████████████████████████████████| 246 kB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GroAIn3YVNzs"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "         .master(\"local\")\\\n",
        "         .appName(\"Colab\")\\\n",
        "         .config('spark.ui.port', '4050')\\\n",
        "         .getOrCreate()\n",
        "# Raw Package\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col\n",
        "#Data Source\n",
        "import yfinance as yf\n",
        "#Data viz\n",
        "import plotly.graph_objs as go\n",
        "import pandas_datareader as web\n",
        "import pyspark.sql.functions as F\n",
        "import time\n",
        "from pyspark.sql.window import Window\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from json import dumps,loads\n",
        "from kafka import KafkaProducer\n",
        "import pandas as pd\n",
        "import pandas_datareader as web\n",
        "from time import sleep\n",
        "\n",
        "producer = KafkaProducer(bootstrap_servers=['192.168.1.15:9092'],api_version=(0,11,5),value_serializer=lambda x:dumps(x).encode('utf-8'))\n",
        "df = yf.download(tickers='AAPL', period='2d', interval='1m')\n",
        "date= df.index\n",
        "df['Date']=date\n",
        "print(df.sample(1).to_dict(orient='records')[0])\n",
        "while True:\n",
        "  dict_stock = df.sample(1).to_dict(orient='records')[0]\n",
        "  producer.send('RTSTCKDATA',value=dict_stock)\n",
        "  sleep(1)\n",
        "\n",
        "producer.flush()\n",
        "\n",
        "producer.send('demo_test', value ={'msg': 'all is well'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "hXD_CVEzx1Qo",
        "outputId": "7b2d2b00-21dc-4d81-a604-9328d519e930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "{'Open': 142.77000427246094, 'High': 143.0800018310547, 'Low': 142.75120544433594, 'Close': 142.9600067138672, 'Adj Close': 142.9600067138672, 'Volume': 354198, 'Date': Timestamp('2022-12-09 10:31:00')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n",
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n",
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n",
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n",
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n",
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n",
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n",
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n",
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n",
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n",
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n",
            "ERROR:kafka.conn:Connection attempt to <BrokerConnection node_id=bootstrap-0 host=192.168.1.15:9092 <connecting> [IPv4 ('192.168.1.15', 9092)]> timed out\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KafkaTimeoutError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKafkaTimeoutError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c791ca7ef228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mdict_stock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'records'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mproducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RTSTCKDATA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_stock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/kafka/producer/kafka.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, topic, value, key, headers, partition, timestamp_ms)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mkey_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_on_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_block_ms'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             key_bytes = self._serialize(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/kafka/producer/kafka.py\u001b[0m in \u001b[0;36m_wait_on_metadata\u001b[0;34m(self, topic, max_wait)\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmetadata_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                 raise Errors.KafkaTimeoutError(\n\u001b[0m\u001b[1;32m    703\u001b[0m                     \"Failed to update metadata after %.1f secs.\" % (max_wait,))\n\u001b[1;32m    704\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munauthorized_topics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKafkaTimeoutError\u001b[0m: KafkaTimeoutError: Failed to update metadata after 60.0 secs."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Interval required 5 minutes\n",
        "data = yf.download(tickers='AAPL', period='2d', interval='1m')\n",
        "date = data.index\n",
        "data['Date']=date\n",
        "sdata = spark.createDataFrame(data) \n",
        "sdata.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pRK3WeQW3ud",
        "outputId": "bdd036d2-72a6-4c6c-d815-a0175db5e552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "root\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            " |-- Volume: long (nullable = true)\n",
            " |-- Date: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def movingAverage(inDF, colName, window):\n",
        "  outDF = None  \n",
        "  inDFI = inDF.withColumn(\"Id\",F.monotonically_increasing_id())\n",
        "   #null_condition = F.col(colName).isNull()\n",
        "  #inDFP = inDFT.withColumn(\"percent_rank\",F.when(null_condition, F.lit(None)).otherwise( F.percent_rank().over(Window.partitionBy(inDFT['City'],F.when(null_condition, 1).otherwise(0)).orderBy(inDFT[colName]))))\n",
        "  windowspec = Window.partitionBy().orderBy(\"Date\").rowsBetween(Window.currentRow-window,Window.currentRow)\n",
        "  outDF=inDFI.withColumn('MovingAvg'+colName, F.format_number((F.mean(colName).over(windowspec)),3))\n",
        "  outDF.show()\n",
        "  return outDF\n",
        "  \n",
        "def sumdata(inDF, colName, window):\n",
        "  outDF = None  \n",
        "  inDFI = inDF.withColumn(\"Id\",F.monotonically_increasing_id())\n",
        "   #null_condition = F.col(colName).isNull()\n",
        "  #inDFP = inDFT.withColumn(\"percent_rank\",F.when(null_condition, F.lit(None)).otherwise( F.percent_rank().over(Window.partitionBy(inDFT['City'],F.when(null_condition, 1).otherwise(0)).orderBy(inDFT[colName]))))\n",
        "  windowspec = Window.partitionBy().orderBy(\"Date\").rowsBetween(Window.currentRow-window,Window.currentRow)\n",
        "  outDF=inDFI.withColumn('Sum'+colName, F.format_number((F.sum(colName).over(windowspec)),3))\n",
        "  outDF.show()\n",
        "  return outDF "
      ],
      "metadata": {
        "id": "VwLgPfgBOzp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DF = movingAverage(sdata,'Close',60)\n",
        "DF1 = sumdata(sdata,'Volume',30)\n",
        "print(DF1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx9hCROEqFNw",
        "outputId": "8b2d37f7-d5a7-4840-b903-2f6690b5b46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------------------+------------------+------------------+------------------+-------+-------------------+---+--------------+\n",
            "|              Open|              High|               Low|             Close|         Adj Close| Volume|               Date| Id|MovingAvgClose|\n",
            "+------------------+------------------+------------------+------------------+------------------+-------+-------------------+---+--------------+\n",
            "|142.36000061035156|142.66749572753906|142.11000061035156| 142.5050048828125| 142.5050048828125|1838425|2022-12-08 09:30:00|  0|       142.505|\n",
            "|142.50289916992188|142.55999755859375|142.02000427246094|142.38999938964844|142.38999938964844| 286139|2022-12-08 09:31:00|  1|       142.448|\n",
            "|142.37159729003906| 142.4199981689453|             142.0|142.02000427246094|142.02000427246094| 186244|2022-12-08 09:32:00|  2|       142.305|\n",
            "|142.00999450683594|142.02000427246094|141.80999755859375| 141.9282989501953| 141.9282989501953| 216083|2022-12-08 09:33:00|  3|       142.211|\n",
            "|141.92999267578125|142.14630126953125| 141.8300018310547|142.09500122070312|142.09500122070312| 227059|2022-12-08 09:34:00|  4|       142.188|\n",
            "|142.08999633789062| 142.1656951904297|141.92999267578125|142.10000610351562|142.10000610351562| 231037|2022-12-08 09:35:00|  5|       142.173|\n",
            "| 142.0800018310547|142.11000061035156|141.51499938964844|141.62010192871094|141.62010192871094| 273290|2022-12-08 09:36:00|  6|       142.094|\n",
            "|141.63499450683594|141.72000122070312|141.52000427246094|141.52499389648438|141.52499389648438| 296257|2022-12-08 09:37:00|  7|       142.023|\n",
            "|141.52999877929688|141.67999267578125|             141.5|141.61000061035156|141.61000061035156| 194850|2022-12-08 09:38:00|  8|       141.977|\n",
            "|141.61160278320312|141.83999633789062| 141.6092987060547|141.74000549316406|141.74000549316406| 246658|2022-12-08 09:39:00|  9|       141.953|\n",
            "|            141.75|141.99000549316406|141.73500061035156| 141.9709014892578| 141.9709014892578| 208202|2022-12-08 09:40:00| 10|       141.955|\n",
            "|141.97500610351562|142.02999877929688|141.74000549316406| 141.9250030517578| 141.9250030517578| 292064|2022-12-08 09:41:00| 11|       141.952|\n",
            "|141.94000244140625| 142.0399932861328| 141.8300018310547| 141.9949951171875| 141.9949951171875| 255229|2022-12-08 09:42:00| 12|       141.956|\n",
            "| 141.9949951171875|             142.0|141.41000366210938|141.41000366210938|141.41000366210938| 255357|2022-12-08 09:43:00| 13|       141.917|\n",
            "|141.41099548339844| 141.4698944091797| 141.2100067138672| 141.2100067138672| 141.2100067138672| 337577|2022-12-08 09:44:00| 14|       141.870|\n",
            "|141.21499633789062| 141.2899932861328|141.10000610351562|141.28500366210938|141.28500366210938| 251764|2022-12-08 09:45:00| 15|       141.833|\n",
            "|141.27000427246094|141.50999450683594|141.25999450683594|141.49000549316406|141.49000549316406| 197228|2022-12-08 09:46:00| 16|       141.813|\n",
            "|             141.5| 141.5500030517578|141.42999267578125|141.49000549316406|141.49000549316406| 173339|2022-12-08 09:47:00| 17|       141.795|\n",
            "|141.48069763183594|141.52000427246094|141.30999755859375|141.44009399414062|141.44009399414062| 211907|2022-12-08 09:48:00| 18|       141.776|\n",
            "|141.44500732421875|141.52000427246094|141.35000610351562| 141.4499969482422| 141.4499969482422| 165141|2022-12-08 09:49:00| 19|       141.760|\n",
            "+------------------+------------------+------------------+------------------+------------------+-------+-------------------+---+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+------------------+------------------+------------------+------------------+------------------+-------+-------------------+---+-------------+\n",
            "|              Open|              High|               Low|             Close|         Adj Close| Volume|               Date| Id|    SumVolume|\n",
            "+------------------+------------------+------------------+------------------+------------------+-------+-------------------+---+-------------+\n",
            "|142.36000061035156|142.66749572753906|142.11000061035156| 142.5050048828125| 142.5050048828125|1838425|2022-12-08 09:30:00|  0|1,838,425.000|\n",
            "|142.50289916992188|142.55999755859375|142.02000427246094|142.38999938964844|142.38999938964844| 286139|2022-12-08 09:31:00|  1|2,124,564.000|\n",
            "|142.37159729003906| 142.4199981689453|             142.0|142.02000427246094|142.02000427246094| 186244|2022-12-08 09:32:00|  2|2,310,808.000|\n",
            "|142.00999450683594|142.02000427246094|141.80999755859375| 141.9282989501953| 141.9282989501953| 216083|2022-12-08 09:33:00|  3|2,526,891.000|\n",
            "|141.92999267578125|142.14630126953125| 141.8300018310547|142.09500122070312|142.09500122070312| 227059|2022-12-08 09:34:00|  4|2,753,950.000|\n",
            "|142.08999633789062| 142.1656951904297|141.92999267578125|142.10000610351562|142.10000610351562| 231037|2022-12-08 09:35:00|  5|2,984,987.000|\n",
            "| 142.0800018310547|142.11000061035156|141.51499938964844|141.62010192871094|141.62010192871094| 273290|2022-12-08 09:36:00|  6|3,258,277.000|\n",
            "|141.63499450683594|141.72000122070312|141.52000427246094|141.52499389648438|141.52499389648438| 296257|2022-12-08 09:37:00|  7|3,554,534.000|\n",
            "|141.52999877929688|141.67999267578125|             141.5|141.61000061035156|141.61000061035156| 194850|2022-12-08 09:38:00|  8|3,749,384.000|\n",
            "|141.61160278320312|141.83999633789062| 141.6092987060547|141.74000549316406|141.74000549316406| 246658|2022-12-08 09:39:00|  9|3,996,042.000|\n",
            "|            141.75|141.99000549316406|141.73500061035156| 141.9709014892578| 141.9709014892578| 208202|2022-12-08 09:40:00| 10|4,204,244.000|\n",
            "|141.97500610351562|142.02999877929688|141.74000549316406| 141.9250030517578| 141.9250030517578| 292064|2022-12-08 09:41:00| 11|4,496,308.000|\n",
            "|141.94000244140625| 142.0399932861328| 141.8300018310547| 141.9949951171875| 141.9949951171875| 255229|2022-12-08 09:42:00| 12|4,751,537.000|\n",
            "| 141.9949951171875|             142.0|141.41000366210938|141.41000366210938|141.41000366210938| 255357|2022-12-08 09:43:00| 13|5,006,894.000|\n",
            "|141.41099548339844| 141.4698944091797| 141.2100067138672| 141.2100067138672| 141.2100067138672| 337577|2022-12-08 09:44:00| 14|5,344,471.000|\n",
            "|141.21499633789062| 141.2899932861328|141.10000610351562|141.28500366210938|141.28500366210938| 251764|2022-12-08 09:45:00| 15|5,596,235.000|\n",
            "|141.27000427246094|141.50999450683594|141.25999450683594|141.49000549316406|141.49000549316406| 197228|2022-12-08 09:46:00| 16|5,793,463.000|\n",
            "|             141.5| 141.5500030517578|141.42999267578125|141.49000549316406|141.49000549316406| 173339|2022-12-08 09:47:00| 17|5,966,802.000|\n",
            "|141.48069763183594|141.52000427246094|141.30999755859375|141.44009399414062|141.44009399414062| 211907|2022-12-08 09:48:00| 18|6,178,709.000|\n",
            "|141.44500732421875|141.52000427246094|141.35000610351562| 141.4499969482422| 141.4499969482422| 165141|2022-12-08 09:49:00| 19|6,343,850.000|\n",
            "+------------------+------------------+------------------+------------------+------------------+-------+-------------------+---+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "DataFrame[Open: double, High: double, Low: double, Close: double, Adj Close: double, Volume: bigint, Date: timestamp, Id: bigint, SumVolume: string]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputDF= spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"192.168.1.15:9092\").option('subscribe','RTSTCKDATA').load()\n",
        "rows = inputDF.select('Volume','Date')\n",
        "StreamingQueryDF = (rows.groupBy('Volume',Window('Date','15 minute').sum()).writeStream.format(\"console\").outputMode('complete').trigger(processingTime='1 second').start())\n",
        "StreamingQueryDF.awaitTermination()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "J3T3v6psyf2V",
        "outputId": "f8b8f1b5-eae1-465a-dcb6-adc687ab19db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d23e35550c58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputDF\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka.bootstrap.servers\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"192.168.1.15:9092\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subscribe'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RTSTCKDATA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Volume'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mStreamingQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Volume'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'15 minute'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'console'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1 second'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mStreamingQuery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".;"
          ]
        }
      ]
    }
  ]
}